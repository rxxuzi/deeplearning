# 損失関数 (Loss Function)

損失関数、またはコスト関数としても知られている、は機械学習モデルがどれだけ正確または間違っているかを量的に評価するための関数である。正確な予測を行った場合、損失は小さくなり、間違った予測を行った場合、損失は大きくなる。以下は、いくつかの一般的な損失関数に関する説明である。

## 1. 平均二乗誤差 (Mean Squared Error: MSE)

平均二乗誤差は、回帰タスクにおいて広く使用される損失関数である。真の値と予測値との差の二乗の平均をとることで計算される。

$$
MSE(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

ここで、$y$ は真の値、$\hat{y}$ はモデルによる予測値、そして $N$ はサンプルの数を示す。

## 2. ソフトマックス交差エントロピー (Softmax Cross Entropy)

ソフトマックス交差エントロピーは、多クラス分類タスクにおける損失関数として使用される。ソフトマックス関数は、クラスの確率的な分布を生成するためのものであり、その後、交差エントロピー損失が計算される。

$$
L(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

ここで、$C$ はクラスの数、$y$ は真のラベルのワンホットエンコーディング、そして $\hat{y}$ はソフトマックス関数を通じて得られる予測確率を示す。

## 3. シグモイド交差エントロピー (Sigmoid Cross Entropy)

シグモイド交差エントロピーは、二値分類タスクのための損失関数である。シグモイド関数を使用して、出力を [0, 1] の範囲に変換した後、交差エントロピー損失が計算される。

$$
L(y, \hat{y}) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
$$

ここで、$y$ は真のラベル（0 または 1）、そして $\hat{y}$ はシグモイド関数を通じて得られる予測確率を示す。

## 4. 二値交差エントロピー (Binary Cross Entropy)

二値交差エントロピーは、シグモイド交差エントロピーと同じく二値分類タスクのための損失関数である。実際、この損失関数はシグモイド交差エントロピーと同一である。

$$
L(y, \hat{y}) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
$$

再び、$y$ は真のラベル（0 または 1）、そして $\hat{y}$ は予測確率を示す。

これらの損失関数は、ニューラルネットワークや他の機械学習アルゴリズムの学習中に、モデルのパラメータを適切に更新するための勾配情報を提供する。モデルの出力と真のラベルとの間の差異を最小化することを目的としている。