# CNN (Convolutional Neural Networks) のメカニズム

CNNは、画像や音声などのグリッド構造データの自動特徴抽出を得意とするニューラルネットワークの一種である。主に以下の層から構成される。

## 1. Convolutionレイヤ (畳み込みレイヤ)

畳み込みレイヤはCNNの核心となる部分で、ローカルな情報をキャッチする役割がある。これは、入力データに対してフィルタ (またはカーネル) をスライドさせて畳み込みを行うことで実現される。

### 数式:

畳み込み操作は以下の式で表される。
$$
O_{i,j} = \sum_{m} \sum_{n} I_{i+m, j+n} \times K_{m, n}
$$
ここで、$O_{i,j}$ は出力のピクセル、$I_{i+m, j+n}$ は入力のピクセル、$K_{m, n}$ はフィルタの値を表す。

### 例:

```text

|1|2|3|0|       |2|0|1|         
|0|1|2|3|       |0|1|2|         |15|16|
|3|0|1|2|       |1|0|2|         | 6|15|
|2|3|0|1|

入力データ       フィルタ        出力データ
```

また、全結合ニューラルネットワークでは、重みパラメータの他に**バイアス**も存在する。

```text

|1|2|3|0|       |2|0|1|         
|0|1|2|3|       |0|1|2|         |15|16|       |4|       |19|20|
|3|0|1|2|       |1|0|2|         | 6|15|                 |10|19|
|2|3|0|1|

入力データ       フィルタ       出力データ     バイアス     出力
``````

フィルタの動き:

<img alt="filter" src="../pics/filter.gif" width="400"/>

### 1.2. **パディング (Padding)**

畳み込み操作を行う際に、入力データの周辺に仮のデータ (通常0) を追加すること。これにより、出力データのサイズの縮小を防ぐか、または調整することができる。

本来の入力データと同じ大きさの出力が得られる

~~~text

|0|0|0|0|0|0|
|0|1|2|3|0|0|       |2|0|1|         | 7|12|10| 2|
|0|0|1|2|3|0|       |0|1|2|         | 4|15|16|10|
|0|3|0|1|2|0|       |1|0|2|         |10| 6|15| 6|
|0|2|3|0|1|0|                       | 8|10| 4| 3|
|0|0|0|0|0|0|

入力データ           フィルタ           出力データ
(padding :1)
~~~

### 1.3.**ストライド (Stride)**

フィルタをスライドさせる際のステップのサイズ。ストライドが1の場合、フィルタは入力データ上で1ピクセルずつスライドする。ストライドが2の場合は2ピクセルずつとなる。

ストライドを大きくすると出力サイズは小さくなる。また、パディングを大きくすると出力サイズは大きくなる。

### 1.4.出力サイズの計算

出力サイズ : $O$ 
入力サイズ : $I$ 
フィルタ(カーネル) : $K$
ストライド : $S$
パディング : $P$

とすると、出力サイズは

$$
O = \frac{I + 2P - K}{S} + 1  
$$

で表すことができる

## 2. Poolingレイヤ (プーリングレイヤ)

プーリングレイヤは、畳み込みによって抽出された特徴のサイズを縮小することで計算量を減らし、特徴の位置感度を低くする役割がある。

### **ダウンサンプリング (Downsampling)**

特徴マップの次元を削減する操作。Max Poolingなどのプーリング操作はダウンサンプリングの一形態として考えられる。

### 例:

2x2 Max Poolingの場合、2x2の領域の中の最大値がその領域の出力となる。

## 3. ReLU (Rectified Linear Unit)

ReLUは、非線形性をネットワークに導入するための活性化関数の一つだ。これは、負の入力値に対しては0を、正の入力値に対してはそのままの値を出力するというシンプルな関数だが、深いネットワークでの学習を助ける特性を持つ。


### 3.1. 数式:

ReLU関数は以下の式で表される。
$$
f(x) = \max(0, x)
$$

### **局所コントラスト正規化 (Local Contrast Normalization)**

畳み込みによって得られた特徴マップの各要素を、その近傍の要素の平均や標準偏差を用いて正規化する操作。これにより、特徴の局所的なコントラストが強調される。

## 4. ミニバッチ処理 (Mini-batch processing)

大量のデータセットに対する学習を効率化するため、一度に小さなサブセット（ミニバッチ）を取り出して学習を行う方法。ミニバッチごとの勾配の平均を用いてパラメータの更新を行う。

---

## 参考:

>一文看懂variant convolutions
> <https://blog.csdn.net/WANGWUSHAN/article/details/103575060>