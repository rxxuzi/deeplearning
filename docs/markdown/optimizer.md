# Optimizer

最適化アルゴリズムは、ニューラルネットワークの学習を効率的に進めるための中核的要素である。
これらのアルゴリズムは、損失関数の値を最小化するようにネットワークのパラメータを更新する。
学習率は、この更新の大きさを制御するパラメータであり、適切な値の選定と制御がモデルの性能に大きく影響する。

学習率が大きすぎると、損失関数の最小値をオーバーシュートして学習が不安定になる可能性があり、小さすぎると学習が非常に遅くなるか、局所的な最小値に陥る恐れがある。
これを解決するために、固定の学習率を使用する代わりに、学習の進行に応じて学習率を動的に調整する方法が提案されている。

## 1. SGD (Stochastic Gradient Descent)

SGDは最も基本的な最適化アルゴリズムで、以下の式に従ってパラメータの更新を行う。
$$
w_{t+1} = w_t - \eta \nabla E(w_t)
$$
ここで、$\eta$ は学習率、$\nabla E(w_t)$ は損失関数の勾配を表す。

## 2. Momentum

MomentumはSGDを拡張した方法で、過去の勾配の移動平均を考慮して重みの更新を行う。
$$
v_{t+1} = \gamma v_t + \eta \nabla E(w_t)
$$
$$
w_{t+1} = w_t - v_{t+1}
$$
ここで、$\gamma$ はモメンタム係数、$v$ は移動平均を表す。

## 3. AdaGrad

AdaGradは学習率をパラメータごとに動的に調整するアルゴリズムである。
$$
h_{t+1} = h_t + (\nabla E(w_t))^2
$$
$$
w_{t+1} = w_t - \frac{\eta}{\sqrt{h_{t+1} + \epsilon}} \nabla E(w_t)
$$
ここで、$h$ は過去の勾配の二乗和、$\epsilon$ は数値的安定性を保つための非常に小さい定数を表す。

## 4. RMSProp (Adadelta)

RMSPropはAdaGradの発展形で、過去のすべての勾配の二乗を考慮するのではなく、最近の勾配の二乗の移動平均を使用する。
$$
h_{t+1} = \beta h_t + (1-\beta) (\nabla E(w_t))^2
$$
$$
w_{t+1} = w_t - \frac{\eta}{\sqrt{h_{t+1} + \epsilon}} \nabla E(w_t)
$$
ここで、$\beta$ は移動平均の係数を表す。


## 5. Adam 

Adam（Adaptive Moment Estimation）は、勾配の1次モーメント（平均）と2次モーメント（未中心分散）の推定に基づく確率的最適化アルゴリズムである。このアルゴリズムは、MomentumとRMSPropの特性を組み合わせており、非常に効果的であると広く認識されている。

### 5.1.  1次モーメントと2次モーメントの移動平均

1次モーメントは勾配の移動平均を表し、2次モーメントは勾配の二乗の移動平均を示す。Adamでは、これらのモーメントを推定するために指数移動平均を使用する。

1. **1次モーメント (m):** 勾配の移動平均。  
   
$$
m_{t+1} = \beta_1 m_t + (1-\beta_1) \nabla E(w_t)
$$

2. **2次モーメント (v):** 勾配の二乗の移動平均。  

$$
v_{t+1} = \beta_2 v_t + (1-\beta_2) (\nabla E(w_t))^2
$$

ここで、$\nabla E(w_t)$ は時刻 $t$ での勾配、$\beta_1$ と $\beta_2$ は移動平均の減衰係数を表す。通常、これらの係数はそれぞれ約0.9と0.999に設定される。

### 5.2. バイアス補正

初期のステップでは、モーメントは0から始まるため、低く見積もられる傾向がある。これを補正するため、Adamではバイアス補正を使用する。

1. **1次モーメントのバイアス補正:** 

$$
\hat{m}_{t+1} = \frac{m_{t+1}}{1 - \beta_1^{t+1}}
$$

2. **2次モーメントのバイアス補正:** 

$$
\hat{v}_{t+1} = \frac{v_{t+1}}{1 - \beta_2^{t+1}}
$$

### 5.3. 重みの更新

バイアス補正を行ったモーメントを使用して、次のように重みを更新する。

$$
w_{t+1} = w_t - \frac{\eta \hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} + \epsilon}
$$

ここで、$\eta$ は学習率、$\epsilon$ は数値的安定性を保つための非常に小さい定数（例: $10^{-7}$）を示す。


Adamは、多様なデータセットとアーキテクチャで良好な性能を発揮し、深いニューラルネットワークの訓練に特に適している。適切なパラメータ設定と共に、多くのアプリケーションでデフォルトの最適化アルゴリズムとして使用されている。


----

>【最適化手法】SGD・Momentum・AdaGrad・RMSProp・Adamを図と数式で理解しよう<https://kunassy.com/oprimizer/>

> SGD、Momentum、RMSprop、Adam区别与联系 - 知乎. <https://zhuanlan.zhihu.com/p/32488889>

> ニューラルネットワークにおける最適化手法 <https://qiita.com/Fumio-eisan/items/798351e4915e4ba396c2>.

>(3) 【決定版】スーパーわかりやすい最適化アルゴリズム -損失関数><https://qiita.com/omiita/items/1735c1d048fe5f611f80>.
