# Function

## 1. シグモイド関数(Sigmoid Function)

$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

シグモイド関数は、実数値の入力を0と1の間に収める非線形関数である。出力は確率として解釈されることが多い。

**使用される場所**: 二項分類問題におけるニューラルネットワークの出力層で、確率を出力するために使用される。

## 2. ReLU関数(Rectified Linear Unit)

$$ ReLU(x) = max(0, x) $$

ReLU関数は、負の入力に対しては0を出力し、正の入力に対してはそのままの値を出力する。これにより、モデルの非線形性を導入しつつ計算効率を保つことができる。

**使用される場所**: 多くのディープラーニングモデルの隠れ層で活性化関数として用いられる。

## 3. ソフトマックス関数(Softmax Function)

$$ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} $$

ソフトマックス関数は、ベクトルを受け取り、要素の合計が1になるような確率分布を出力する。多クラス分類問題での確率解釈に適している。

**使用される場所**: 多クラス分類問題のニューラルネットワーク出力層で、クラスの確率を出力するために使用される。

## 4. 平均二乗誤差 (MSE , Mean Squared Error)

$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$
平均二乗誤差は、予測値と実際の値の差の二乗の平均を取ることで、予測の誤差を評価する指標である。

**使用される場所**: 回帰問題におけるニューラルネットワークの損失関数としてよく使用される。

## 5. クロスエントロピー損失 (Cross-Entropy Loss)

$$ CE = -\sum_{c=1}^{M} y_{o,c} \log(p_{o,c}) $$
$$ \text{where } y_{o,c} \text{ is the true label and } p_{o,c} \text{ is the predicted probability.} $$

クロスエントロピー損失は、予測された確率分布と実際の分布との差を測る。分類問題においてモデルの性能を評価するのに適している。

**使用される場所**: 分類問題、特に多クラス分類問題のニューラルネットワークの損失関数として使用される。

## 6. ハイパボリックタンジェント (Hyperbolic Tangent Function)

$$ \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$

タンジェントハイパボリック関数は、実数値の入力を-1と1の間に収める非線形関数であり、シグモイド関数と似ているが、より広い出力範囲を持つ。

**使用される場所**: シグモイド関数と同様に、ニューラルネットワークの隠れ層で活性化関数として用いられることがある。